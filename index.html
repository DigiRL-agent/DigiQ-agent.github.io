<!DOCTYPE html>
<html>

<head>
    <title>Digi-Q</title>
    <link rel="icon" href="website/images/icon/icon.png" type="image/icon type">

    <meta name="viewport" content="width=device-width, initial-scale=1">

    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-datalabels@2.0.0"></script>
    <script
        src="https://cdn.jsdelivr.net/npm/chartjs-plugin-annotation@3.0.1/dist/chartjs-plugin-annotation.min.js"></script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <link rel="stylesheet" href="website/css/bulma.min.css">
    <link rel="stylesheet" href="website/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="website/css/bulma-slider.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="./website/javascript/bulma-carousel.min.js"></script>
    <script src="./website/javascript/bulma-slider.min.js"></script>

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p"
        crossorigin="anonymous"></script>

    <link href="https://unpkg.com/tabulator-tables@5.5.2/dist/css/tabulator_bootstrap4.min.css" rel="stylesheet">
    <script type="text/javascript" src="https://unpkg.com/tabulator-tables@5.5.2/dist/js/tabulator.min.js"></script>
    <!-- <script src="website/javascript/peity-vanilla.js"></script> -->

    <script src="website/javascript/benchmark_table.js" type="module"></script>
    <script src="website/javascript/success_rate_vs_k_vis.js" type="module"></script>
    <script src="website/javascript/success_rate_vs_k_vis2.js" type="module"></script>
    <script src="website/javascript/feedback_success_rate_vis.js" type="module"></script>
    <script src="website/javascript/feedback_provider_efficacy.js" type="module"></script>
    <script src="website/javascript/demos.js" type="module"></script>

    <link rel="stylesheet" href="website/css/index.css">

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C7GJ4FYMY9"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'G-C7GJ4FYMY9');

    </script>

    

    <noscript>
        <p><img alt="Clicky" width="1" height="1" src="//in.getclicky.com/101339888ns.gif" /></p>
    </noscript>
</head>

<body>

    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title publication-title">
                            <img src="website/images/icon/icon.png" alt="logo" width="40" height="40" />
                            Digi-Q: Learning VLM Q-Value Functions for Training Device-Control Agents
                        </h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="https://www.jackgethome.com/">Hao Bai</a><sup>1,2*</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://yifeizhou02.github.io">Yifei Zhou</a><sup>1*</sup>,
                            </span>
                            <br>
                            <span class="author-block">
                                <a href="https://www.cs.columbia.edu/~lierranli/">Li Erran Li</a><sup>3</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a><sup>1</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://aviralkumar2907.github.io/">Aviral Kumar</a><sup>4,5</sup>
                            </span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><sup>1</sup> UC Berkeley,</span>
                            <span class="author-block"><sup>2</sup> UIUC,</span>
                            <span class="author-block"><sup>3</sup> Amazon,</span>
                            <span class="author-block"><sup>4</sup> CMU,</span>
                            <span class="author-block"><sup>5</sup> Google DeepMind</span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><small>*Equal contribution; work done at UC Berkeley</small></span>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- PDF Link. -->
                                <span class="link-block">
                                    <a href="https://arxiv.org/abs/2406.11896" class="btn btn-outline-dark"
                                        role="button">&#128221;
                                        Paper</a> &nbsp;&nbsp;

                                </span>
                                <!-- Code Link. -->
                                <span class="link-block">
                                    <a href="https://github.com/DigiRL-agent/digirl" class="btn btn-outline-dark"
                                        role="button">&#128187;
                                        Code</a> &nbsp;&nbsp;

                                </span>
                                <!-- Dataset Link. -->
                                <span class="link-block">

                                    <a href="https://drive.google.com/drive/folders/14Iu6lAHePQ2qG0ghYkVG1RG6RUu7e2Hz?usp=sharing"
                                        class="btn btn-outline-dark" role="button">&#128194;
                                        Data</a>
                            </div>
                        </div>

                        <!-- <h2 class="subtitle" style="text-align: left;">
                            <b>MINT benchmark</b> measures LLMs' ability to solve tasks with multi-turn interactions
                            by
                            (1) using tools and (2) leveraging natural language feedback.
                        </h2> -->
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section" id="propaganda">
        <div class="container is-max-desktop">
            <div class="is-centered has-text-centered">
                <div class="custom-column-large">
                    <div class="is-centered custom-column-large">
                        <h2 class="title is-3">Demo</h2>
                        <div class="text-center">
                            <div class="btn-group btn-group-toggle text-center task-selector" data-toggle="buttons">
                                <button type="button" class="btn btn-outline-secondary btn-sm inline-vis-button" disabled>RL Algorithm:</button>
                                <button type="button" class="btn btn-outline-secondary btn-sm active" id="fast-button">Digi-Q</button>
                                <button type="button" class="btn btn-outline-secondary btn-sm" id="slow-button">Other Policy-based Methods</button>
                            </div>
                        </div>
                    </div>
                        <br>
                        <!-- All video -->
                    <div id="fast-container">
                        <video controls autoplay muted loop>
                            <source src="website/videos/animation_digiq.mp4" type="video/mp4">
                        </video>
                    </div>

                    <div id="slow-container" style="display:none;">
                        <video controls autoplay muted loop>
                            <source src="website/videos/animation_policy_based.mp4" type="video/mp4">
                        </video>
                    </div>
                    <small>Success rate and corresponding trajectories. A <font color="#007800">green</font> final screen indicates a successful trajectory; a <font color="#ff1900">red</font> final screen indicates a failed trajectory.</s>
                </div>
            </div>
        </div>
    </section>

    <section class="section" id="abstract">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
While most paradigms for building foundation model agents rely on prompting or fine-tuning
on demonstrations, it is not sufficient in dynamic environments (e.g., mobile device control). On-policy
reinforcement learning (RL) should address these limitations, but collecting actual rollouts in an environment
is often undesirable when using truly open-ended agentic tasks such as mobile device, where simulation is a
bottleneck. In such scenarios, an <b>offline</b> method for policy improvement that utilizes a trained value-function
for training the policy is much more practical. <b>In this paper, we develop a scalable value-based offline RL
approach called Digi-Q to train VLM agents for device control entirely from static data.</b> The key idea in Digi-Q
is to train a value function using <b>offline temporal-difference (TD)</b> learning. We show that this can be done by
fine-tuning a Q-function on top of <b>frozen, intermediate-layer features of a VLM</b> rather than fine-tuning the
whole VLM itself, which saves us compute and enhances training stability. To make the VLM features amenable
for representing the value function, we need to employ an initial phase of fine-tuning to amplify coverage over
actionable information critical for Q-functions. Once trained, we use this value function alongside a best-of-N
policy extraction operator that imitates the best action out of multiple candidate actions from the current policy
as ranked by the value function, enabling policy improvement without ever needing to use the simulator. Digi-Q
outperforms several prior methods on user-scale device control tasks in Android-in-the-Wild, attaining 9.9%
improvement over prior best-performing method.
                        </p>
                    </div>
                </div>
            </div>
            <!--/ Abstract. -->
        </div>
    </section>

    <section class="hero teaser">
        <div class="container is-max-desktop">
            <div class="hero-body">

                <h2 class="subtitle">
                    <b>Digi-Q</b> achieves online-level performance with offline data only, by learning a reliable state-action (Q) value function upon a fixed vision-language model (VLM) after contrastive learning.
                </h2>

                <div class="tab-content" id="myTabContent">
                    <div class="tab-pane fade show active" id="benchmark-table-content" role="tabpanel"
                        aria-labelledby="benchmark-table-content">

                        <div id="benchmark-table"></div>
                    </div>
                    <div class="tab-pane fade" id="eurus-code-table-content" role="tabpanel"
                        aria-labelledby="eurus-code-table-content">

                        <p class="mt-2 px-2">
                            This code subset follows the <a href="https://arxiv.org/abs/2404.02078">Eurus
                                paper</a> and contains MBPP and HumanEval.
                        </p>
                    </div>
                    <p class="mt-2 px-2">
                        This table contains the success rate across all approaches measured in the DigiRL paper. It includes performance on
                        two subsets: AitW General and AitW Web Shopping. The codename of GPT-4V we use is <i>gpt-4-vision-preview</i>
                        and the codename of Gemini-1.5-Pro is <i>gemini-1.5-pro-latest</i>.
                    </p>
                </div>
            </div>
        </div>
    </section>

    <section class="section" id="interaction-framework">
        <div class="container is-max-desktop">

            <div class="columns is-full-width">

                <!-- Visual Effects. -->
                <div class="column">
                    <div class="text-justified">
                        <h2 class="title is-3">DigiRL: Representation Learning for Better Value-based RL</h2>

                        <h3> Why offline RL? </h3>
                        <ul class="custom-bullets">
                            <li>LLM Agent data such as device-control actions is poorly represented in the pre-training corpus of <b>Off-the-shelf proprietary VLMs</b> such as GPT4V and Gemini-1.5-Pro.</li>
                            <li><b>Supervised Fine-Tuning</b> 1) requires a large amount of human demonstration data and 2) cannot recover from degrading model performance when real websites/applications have changed. As shown in the plot below, a frozen good policy trained with prior data experiences a gradual drop in performance as the websites change over time, while the DigiRL policy constantly updates with fresh autonomous data can maintain a stable performance.</li>
                        </ul>
                    </div>
                    <!-- <div class="text-center">
                        <div class="btn-group btn-group-toggle text-center task-selector" data-toggle="buttons">
                            <button type="button" class="btn btn-outline-secondary btn-sm inline-vis-button" disabled
                                id="visualize-sr-vs-k-scale-with-model-size-llama2-base-disabled">Visualization:</button>
                            <button type="button" class="btn btn-outline-secondary btn-sm active"
                                id="exc-offline">Exclude Offline Results (click to visualize)</button>
                            <button type="button" class="btn btn-outline-secondary btn-sm"
                                id="inc-offline">Include Offline Results</button>
                        </div>
                    </div> -->
                    <div class="text-justified">
                        <h3> How did we get Value-based RL work on Device Control?</h3>
                        Digi-Q consists of two steps:
                        <ul class="custom-bullets">
                            <li>First, we use <b>Representation Fine-tuning</b> to make the VLM aware of what it needs to focus on.</li>
                            <li>Then, we use <b>Offline-to-Online RL</b> to encourage the agent to learn from its own trials and errors.</li>
                        </ul>
                        DigiRL identifies the most simple yet effective RL design choices for device-control agent problems. Our RL algorithmic framework automatically achieves the following advantages compared to state-of-the-art alternatives such as rejection sampling (or Filtered Behavior Cloning):
                        <ul class="custom-bullets">
                            <li>We makes use of an <b>instruction-level value function</b> to implicitly construct an automatic curriculum that prioritizes on the tasks most informative to the agent.</li>
                            <li>We makes use of a <b>step-level value function</b> to pick out the advantageous actions (actions that mark progress towards the goal) in a trajectory while leaving the noisy actions (actions that do not contribute to the goal).</li>
                        </ul>
                        Please check out our paper for more details of our algorithm!
                        <br><br>
                        <div style="text-align:center;">
                            <img src="website/images/algo.png" alt="illustrative-example"
                                style="margin: 0 auto; display: block; max-width: 1000px; width: 100%; height: auto;" />
                            <br>
                        </div>
                        <h3>Q Values We Observed</h3>
                        <div class="text-justified" id="tool-augmented">
                            We randomly picked some states and examine Q values of different actions over these states.
                        </div>
                        
                    </div>
                </div>
                <!--/ Visual Effects. -->

            </div>
    </section>

    <!-- <section class="section" id="evaluation">
        <div class="container is-max-desktop">

            <div class="columns is-full-width">

                <div class="column">
                    <div class="content">
                        <h3 class="title is-3">Autonomous Evaluation</h3>
                        <p>
                            Our main results are autonomously evaluated with Gemini-1.5-Pro. We also manually evaluate on some subsets and finds that the autonomous evaluation results highly align with manual evaluations with an average difference less than 3%:
                        </p>

                        <div class="text-center">
                            <div class="btn-group btn-group-toggle text-center feedback-provider-sort-by-selector"
                                data-toggle="buttons">
                                <button type="button" class="btn btn-outline-secondary btn-sm" disabled>Subset:</button>
                                <button type="button" class="btn btn-outline-secondary btn-sm active"
                                    id="sort-by-feedback-gain">
                                    AitW General
                                </button>

                                <button type="button" class="btn btn-outline-secondary btn-sm"
                                    id="sort-by-feedback-provider-perf">
                                    AitW Web Shopping
                                </button>
                            </div>
                        </div>

                        <div class="chart-container" id="chart-feedback-p" style="display:block;margin:0 auto;">
                            <canvas id="chart-feedback-provider"></canvas>
                        </div>

                        <h3>Failure Mode Analysis</h3>

                        <p>
                            While all the types of failure modes benefit from offline and offline-to-online RL training, the most consistent and significant reduction is for the failure mode of failing to recover from mistakes. By training on autonomously-collected rollouts, our agent DigiRL is able to learn from its own mistakes and reduces failures to recover over training.
                        </p>

                        <div class="text-center">
                            <div class="btn-group btn-group-toggle text-center task-selector" data-toggle="buttons">
                                <button type="button" class="btn btn-outline-secondary btn-sm" disabled>Subset:</button>
                                <button type="button" class="btn btn-outline-secondary btn-sm active"
                                    id="avg_micro">AitW General</button>
                                <button type="button" class="btn btn-outline-secondary btn-sm"
                                    id="reasoning">AitW Web Shopping</button>
                            </div>


                            <div class="btn-group btn-group-toggle text-center sort-by-selector" data-toggle="buttons">
                                <button type="button" class="btn btn-outline-secondary btn-sm" disabled>Failure:</button>
                                <button type="button" class="btn btn-outline-secondary btn-sm active"
                                    id="sort-by-feedbacksr">Fail to recover from mistakes</button>
                                <button type="button" class="btn btn-outline-secondary btn-sm"
                                    id="sort-by-nofeedbacksr">Get stuck midway</button>
                                <button type="button" class="btn btn-outline-secondary btn-sm"
                                    id="sort-by-feedbackdelta">Arrive at wrong goal</button>
                            </div> 
                        </div>

                        <div class="chart-container" id="chart-feedback" style="position:relative;margin:0 auto;">
                            <canvas id="chart-sr-w-feedback" style="max-height: 100%;"></canvas>
                        </div> 

                    </div>
                </div>
            </div>
    </section> -->

    <section class="section" id="evaluation">
        <div class="container is-max-desktop">
            <div class="columns is-full-width">
                <div class="column">
                    <div class="content">
                        <h3>Misc</h3>
                        <h4>Re the Icon <img src="website/images/icon/icon.png" alt="logo" width="30" height="30" /></h4>
                        <p>
                            <b>Q:</b> The “Q” hat upon the Android is Chinese brush calligraphy, with a hidden “∞” inside, which relates <a href="https://digirl-agent.github.io/">DigiRL</a> (where the Android has a hat of “∞”)
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre><code>@article{bai2025digiq,
title={Digi-Q: Learning VLM Q-Value Functions for Training Device-Control Agents},
author={Bai, Hao and Zhou, Yifei and Li, Erran Li and Levine, Sergey and Kumar, Aviral},
journal={},
year={2025}
}</code></pre>
        </div>
    </section>

    <footer class="footer">
        <div align="center" class="container">
            <div class="columns is-centered">
                <div class="content is-small">
                    This website templated is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">here</a> and <a href="https://xwang.dev/mint-bench/">here</a>.
                </div>
            </div>
        </div>
    </footer>

</body>


</html>
