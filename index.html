<!DOCTYPE html>
<html>

<head>
    <title>Digi-Q</title>
    <link rel="icon" href="website/images/icon/icon.png" type="image/icon type">

    <meta name="viewport" content="width=device-width, initial-scale=1">

    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-datalabels@2.0.0"></script>
    <script
        src="https://cdn.jsdelivr.net/npm/chartjs-plugin-annotation@3.0.1/dist/chartjs-plugin-annotation.min.js"></script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <link rel="stylesheet" href="website/css/bulma.min.css">
    <link rel="stylesheet" href="website/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="website/css/bulma-slider.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="./website/javascript/bulma-carousel.min.js"></script>
    <script src="./website/javascript/bulma-slider.min.js"></script>

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p"
        crossorigin="anonymous"></script>

    <link href="https://unpkg.com/tabulator-tables@5.5.2/dist/css/tabulator_bootstrap4.min.css" rel="stylesheet">
    <script type="text/javascript" src="https://unpkg.com/tabulator-tables@5.5.2/dist/js/tabulator.min.js"></script>
    <!-- <script src="website/javascript/peity-vanilla.js"></script> -->

    <script src="website/javascript/benchmark_table.js" type="module"></script>
    <script src="website/javascript/success_rate_vs_k_vis.js" type="module"></script>
    <script src="website/javascript/success_rate_vs_k_vis2.js" type="module"></script>
    <script src="website/javascript/feedback_success_rate_vis.js" type="module"></script>
    <script src="website/javascript/feedback_provider_efficacy.js" type="module"></script>
    <script src="website/javascript/demos.js" type="module"></script>

    <link rel="stylesheet" href="website/css/index.css">

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-C7GJ4FYMY9"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'G-C7GJ4FYMY9');

    </script>

    

    <noscript>
        <p><img alt="Clicky" width="1" height="1" src="//in.getclicky.com/101339888ns.gif" /></p>
    </noscript>
</head>

<body>

    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title publication-title">
                            <img src="website/images/icon/icon.png" alt="logo" width="40" height="40" />
                            Digi-Q: Learning VLM Q-Value Functions for Training Device-Control Agents
                        </h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="https://www.jackgethome.com/">Hao Bai</a><sup>1,2*</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://yifeizhou02.github.io">Yifei Zhou</a><sup>1*</sup>,
                            </span>
                            <br>
                            <span class="author-block">
                                <a href="https://www.cs.columbia.edu/~lierranli/">Li Erran Li</a><sup>3</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a><sup>1</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://aviralkumar2907.github.io/">Aviral Kumar</a><sup>4</sup>
                            </span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><sup>1</sup> UC Berkeley,</span>
                            <span class="author-block"><sup>2</sup> UIUC,</span>
                            <span class="author-block"><sup>3</sup> Amazon,</span>
                            <span class="author-block"><sup>4</sup> CMU</span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><small>*Equal contribution</small></span>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- PDF Link. -->
                                <span class="link-block">
                                    <a href="https://arxiv.org/abs/2406.11896" class="btn btn-outline-dark"
                                        role="button">&#128221;
                                        Paper</a> &nbsp;&nbsp;

                                </span>
                                <!-- Code Link. -->
                                <span class="link-block">
                                    <a href="https://github.com/DigiRL-agent/digiq" class="btn btn-outline-dark"
                                        role="button">&#128187;
                                        Code</a> &nbsp;&nbsp;

                                </span>
                                <!-- Dataset Link. -->
                                <span class="link-block">

                                    <a href="https://huggingface.co/collections/JackBAI/digi-q-67a8b0f83a24e14cbfd1fbc6"
                                        class="btn btn-outline-dark" role="button">&#128194;
                                        Data</a>
                            </div>
                        </div>

                        <!-- <h2 class="subtitle" style="text-align: left;">
                            <b>MINT benchmark</b> measures LLMs' ability to solve tasks with multi-turn interactions
                            by
                            (1) using tools and (2) leveraging natural language feedback.
                        </h2> -->
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section" id="propaganda">
        <div class="container is-max-desktop">
            <div class="is-centered has-text-centered">
                <div class="custom-column-large">
                    <div class="is-centered custom-column-large">
                        <h2 class="title is-3">Demo</h2>
                        <div class="text-center">
                            <div class="btn-group btn-group-toggle text-center task-selector" data-toggle="buttons">
                                <button type="button" class="btn btn-outline-secondary btn-sm inline-vis-button" disabled>RL Algorithm:</button>
                                <button type="button" class="btn btn-outline-secondary btn-sm active" id="fast-button">Digi-Q</button>
                                <button type="button" class="btn btn-outline-secondary btn-sm" id="slow-button">Policy-based Methods</button>
                            </div>
                        </div>
                    </div>
                        <br>
                        <!-- All video -->
                    <div id="fast-container">
                        <video controls autoplay muted loop>
                            <source src="website/videos/animation_digiq.mp4" type="video/mp4">
                        </video>
                    </div>

                    <div id="slow-container" style="display:none;">
                        <video controls autoplay muted loop>
                            <source src="website/videos/animation_policy_based.mp4" type="video/mp4">
                        </video>
                    </div>
                    <small>Method overview of Digi-Q compared to policy-based methods. Click on the button to switch the method.</small>
                </div>
            </div>
        </div>
    </section>

    <section class="section" id="abstract">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
While most paradigms for building foundation model agents rely on prompting or fine-tuning
on demonstrations, it is not sufficient in dynamic environments (e.g., mobile device control). On-policy
reinforcement learning (RL) should address these limitations, but collecting actual rollouts in an environment
is often undesirable when using truly open-ended agentic tasks such as mobile device, where simulation is a
bottleneck. In such scenarios, an <b>offline</b> method for policy improvement that utilizes a trained value-function
for training the policy is much more practical. <b>In this paper, we develop a scalable value-based offline RL
approach called Digi-Q to train VLM agents for device control entirely from static data.</b> The key idea in Digi-Q
is to train a value function using <b>offline temporal-difference (TD)</b> learning. We show that this can be done by
fine-tuning a Q-function on top of <b>frozen, intermediate-layer features of a VLM</b> rather than fine-tuning the
whole VLM itself, which saves us compute and enhances training stability. To make the VLM features amenable
for representing the value function, we need to employ an initial phase of fine-tuning to amplify coverage over
actionable information critical for Q-functions. Once trained, we use this value function alongside a best-of-N
policy extraction operator that imitates the best action out of multiple candidate actions from the current policy
as ranked by the value function, enabling policy improvement without ever needing to use the simulator. Digi-Q
outperforms several prior methods on user-scale device control tasks in Android-in-the-Wild, attaining 9.9%
improvement over prior best-performing method.
                        </p>
                    </div>
                </div>
            </div>
            <!--/ Abstract. -->
        </div>
    </section>

    <section class="section" id="interaction-framework">
        <div class="container is-max-desktop">

            <div class="columns is-full-width">

                <!-- Visual Effects. -->
                <div class="column">
                    <div class="text-justified">
                        <h2 class="title is-3">Digi-Q: Training VLM Q-Value Functions for Agentic Policy Learning</h2>

                        <h3> Why do we need to train an off-policy Q function for device-control agents? </h3>
                        <ul class="custom-bullets">
                            <li>While <b>policy-based methods</b> such as <a href="https://arxiv.org/abs/1707.06347">PPO</a> and <a href="https://arxiv.org/abs/2406.11896">DigiRL</a> 
                                can achieve strong performances through sampling a large amount of on-policy data, they may be impractical for real-world device-control tasks where <b>simulation is slow and restricted due to privacy concerns</b>. </li>
                            <li><b>Value-based methods</b> learn an off-policy Q function from historically collected data to score a policy's actions reliably. In this way, we can significanly simplify our recipe for <b>policy improvement without costly simulations.</b> </li>
                        </ul>
                    </div>
                    <div style="text-align: center;">
                        <img src="website/images/teaser.png" width="1000"/> 
                    </div>
                    <!-- <div class="text-center">
                        <div class="btn-group btn-group-toggle text-center task-selector" data-toggle="buttons">
                            <button type="button" class="btn btn-outline-secondary btn-sm inline-vis-button" disabled
                                id="visualize-sr-vs-k-scale-with-model-size-llama2-base-disabled">Visualization:</button>
                            <button type="button" class="btn btn-outline-secondary btn-sm active"
                                id="exc-offline">Exclude Offline Results (click to visualize)</button>
                            <button type="button" class="btn btn-outline-secondary btn-sm"
                                id="inc-offline">Include Offline Results</button>
                        </div>
                    </div> -->
                    <br>
                    <div class="text-justified">
                        <h3> What are the challenges for training an off-policy Q function with foundation models?</h3>
                        <ul class="custom-bullets">
                            <li>(1) <b>Instability</b> in running temporal-difference (TD) learning with large models with billions of parameters.</li>
                            <li>(2) <b>Inefficiency</b> of TD backups per unit amount of "compute" (i.e., gradient steps spent).</li>
                        </ul>
                    </div>
                    <div class="text-justified">
                        <h3> How does Digi-Q deal with these challenges?</h3>
                        <ul class="custom-bullets">
                            <li>Digi-Q trains the Q function on top of a frozen intermediate layer of the VLM, after an initial phase of <b>representation fine-tuning</b> to prime representations of a VLM to be more amenable to TD-learning. . </li>
                            <li>Once the Q function is trained, a <b>Best-of-N policy-extraction objective</b> is applied to train the agentic policy to imitate the best-rated action per the Q-function without any additional environment interactions. </li>
                        </ul>
                    </div>
                    <div style="text-align: center;">
                        <img src="website/images/method.png" width="1000" />
                    </div>
                </div>
                <!--/ Visual Effects. -->
            </div>
    </section>

    <!-- <section class="section" id="evaluation">
        <div class="container is-max-desktop">

            <div class="columns is-full-width">

                <div class="column">
                    <div class="content">
                        <h3 class="title is-3">Autonomous Evaluation</h3>
                        <p>
                            Our main results are autonomously evaluated with Gemini-1.5-Pro. We also manually evaluate on some subsets and finds that the autonomous evaluation results highly align with manual evaluations with an average difference less than 3%:
                        </p>

                        <div class="text-center">
                            <div class="btn-group btn-group-toggle text-center feedback-provider-sort-by-selector"
                                data-toggle="buttons">
                                <button type="button" class="btn btn-outline-secondary btn-sm" disabled>Subset:</button>
                                <button type="button" class="btn btn-outline-secondary btn-sm active"
                                    id="sort-by-feedback-gain">
                                    AitW General
                                </button>

                                <button type="button" class="btn btn-outline-secondary btn-sm"
                                    id="sort-by-feedback-provider-perf">
                                    AitW Web Shopping
                                </button>
                            </div>
                        </div>

                        <div class="chart-container" id="chart-feedback-p" style="display:block;margin:0 auto;">
                            <canvas id="chart-feedback-provider"></canvas>
                        </div>

                        <h3>Failure Mode Analysis</h3>

                        <p>
                            While all the types of failure modes benefit from offline and offline-to-online RL training, the most consistent and significant reduction is for the failure mode of failing to recover from mistakes. By training on autonomously-collected rollouts, our agent DigiRL is able to learn from its own mistakes and reduces failures to recover over training.
                        </p>

                        <div class="text-center">
                            <div class="btn-group btn-group-toggle text-center task-selector" data-toggle="buttons">
                                <button type="button" class="btn btn-outline-secondary btn-sm" disabled>Subset:</button>
                                <button type="button" class="btn btn-outline-secondary btn-sm active"
                                    id="avg_micro">AitW General</button>
                                <button type="button" class="btn btn-outline-secondary btn-sm"
                                    id="reasoning">AitW Web Shopping</button>
                            </div>


                            <div class="btn-group btn-group-toggle text-center sort-by-selector" data-toggle="buttons">
                                <button type="button" class="btn btn-outline-secondary btn-sm" disabled>Failure:</button>
                                <button type="button" class="btn btn-outline-secondary btn-sm active"
                                    id="sort-by-feedbacksr">Fail to recover from mistakes</button>
                                <button type="button" class="btn btn-outline-secondary btn-sm"
                                    id="sort-by-nofeedbacksr">Get stuck midway</button>
                                <button type="button" class="btn btn-outline-secondary btn-sm"
                                    id="sort-by-feedbackdelta">Arrive at wrong goal</button>
                            </div> 
                        </div>

                        <div class="chart-container" id="chart-feedback" style="position:relative;margin:0 auto;">
                            <canvas id="chart-sr-w-feedback" style="max-height: 100%;"></canvas>
                        </div> 

                    </div>
                </div>
            </div>
    </section> -->


    <section class="section" id="evaluation">
        <div class="container is-max-desktop">
            <div class="columns is-full-width">
                <div class="column">
                    <div class="content">
                        <h2 class="title is-3">Experiment results on Android-in-the-Wild</h2>
                        <!-- <h3>Experiment results on Android-in-the-Wild</h3> -->
                        <p>
                            Digi-Q achieves superior performance compared to other state-of-the-art RL baselines using historically collect data. Surprisingly, it is even comparable to the strongest online RL baseline DigiRL
                            , without relying on costly online simulations! Please refer to our paper for more analysis results.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>


    <section class="hero teaser">
        <div class="container is-max-desktop">
            <div class="hero-body">

                <div class="tab-content" id="myTabContent">
                    <div class="tab-pane fade show active" id="benchmark-table-content" role="tabpanel"
                        aria-labelledby="benchmark-table-content">

                        <div id="benchmark-table"></div>
                    </div>
                    <div class="tab-pane fade" id="eurus-code-table-content" role="tabpanel"
                        aria-labelledby="eurus-code-table-content">

                        <p class="mt-2 px-2">
                            This code subset follows the <a href="https://arxiv.org/abs/2404.02078">Eurus
                                paper</a> and contains MBPP and HumanEval.
                        </p>
                    </div>
                    <p class="mt-2 px-2">
                        This table contains the success rate across all approaches measured in our paper. It includes performance on
                        two subsets: AitW General and AitW Web Shopping.
                    </p>
                </div>
            </div>
        </div>
    </section>

    <section class="section" id="evaluation">
        <div class="container is-max-desktop">
            <div class="columns is-full-width">
                <div class="column">
                    <div class="content">
                        <h3>Scaling performance of Digi-Q</h3>
                        <p>
                            We compare the performance of Digi-Q and SOTA offline baseline DigiRL across different scales of offline data, and we found that DigiQ has the following benefits:
                        </p>
                        <ul class="custom-bullets">
                            <li>(1) <b>Data efficiency</b>. By reusing off-policy data from replay buffer, Digi-Q achieves better performance with fewer data.</li>
                            <li>(2) <b>Convergence performance</b>. By performing per-step credit assignment, Digi-Q achieves better convergence performance.</li>
                        </ul>
                    </div>
                </div>

                <div style="text-align: center;">
                    <img src="website/images/data_efficiency.png" width="500"/> 
                </div>
            </div>
        </div>
    </section>

    <section class="section" id="evaluation">
        <div class="container is-max-desktop">
            <div class="columns is-full-width">
                <div class="column">
                    <div class="content">
                        <h3>Misc</h3>
                        <h4>Re the Icon <img src="website/images/icon/icon.png" alt="logo" width="30" height="30" /></h4>
                        <p>
                            <b>Q:</b> The “Q” hat upon the Android is Chinese brush calligraphy, with a hidden “∞” inside, which relates <a href="https://digirl-agent.github.io/">DigiRL</a> (where the Android has a hat of “∞”)
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre><code>@article{bai2025digiq,
title={Digi-Q: Learning VLM Q-Value Functions for Training Device-Control Agents},
author={Bai, Hao and Zhou, Yifei and Li, Erran Li and Levine, Sergey and Kumar, Aviral},
journal={},
year={2025}
}</code></pre>
        </div>
    </section>

    <footer class="footer">
        <div align="center" class="container">
            <div class="columns is-centered">
                <div class="content is-small">
                    This website templated is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">here</a> and <a href="https://xwang.dev/mint-bench/">here</a>.
                </div>
            </div>
        </div>
    </footer>

</body>


</html>
